Las series cronológicas han sido un importante tema de investigación durante décadas [@tsa_decades]. Su objetivo principal consiste en obtener simplificaciones de la realidad mediante el ajuste de diversos modelos, los cuales se ajustan a datos recolectados a lo largo del tiempo de forma periódica.  

Sin embargo, encontrar un modelo que presente un buen comportamiento con respecto a los datos no es sencillo, pues deben considerarse diversos aspectos teóricos, prácticos, y de la temática de estudio para así obtener un modelo adecuado que logre generar pronósticos realistas y pertinentes para la toma de decisiones [@tsa_decision_making].

Una serie temporal se define como una secuencia de datos observados, cuyas mediciones ocurren de manera sucesiva durante un periodo de tiempo. Los registros de estos datos pueden referirse a una única variable en cuyo caso de dice que es una serie univariada. Según @Hipel, cada observación puede ser continua o discreta, como la temperatura de una ciudad durante el día o las variaciones diarias del precio de un activo financiero, respectivamente; las observaciones continuas, además, pueden ser convertidas a su vez en observaciones discretas. De esta manera, una serie de tiempo puede considerarse una muestra aleatoria, pues para un determinado tiempo $t$, que se considera el momento actual, la serie tiene tres momentos: el pasado, que son los rezagos denotados como $Y_{t-1}, Y_{t-2}, \cdots, Y_{t-k}$ donde $k$ es el primer momento de referencia, el momento presente, denotado como $Y_t$, y los pronósticos, denotados como $Y_{t+1}, Y_{t+2}, \cdots, Y_{t+h}$; así, una serie temporal univariada, con lapsos equidistantes entre los tiempos, puede representarse como $Y_{t-k}, \cdots, Y_{t-2}, Y_{t-1}, Y_t, Y_{t+1}, Y_{t+2}, \cdots, Y_{t+h}$. 

A partir de lo anterior, la serie cronológica se compone de dos partes: la estocástica, que contiene una parte conocida (sistemática) y susceptible de predecir y de una parte totalmente desconocida o aleatoria; y una parte determinística, que representa una ecuación matemática sin error, dado que no posee más que ese componente determinístico, se trata de una variable que está determinada o fija y que no cambia de una muestra a otra. De esta manera, puede concluirse que una serie cronológica cuenta con dos características fundamentales: Los valores se encuentran ordenados cronológicamente y, además, existe una dependencia o correlación entre los valores de dicha serie de tiempo; de no presentarse estas dos condiciones, no se estaría en presencia de una serie cronológica. Así, puede decirse que las series de tiempo se enfocan en tres grandes objetivos que serán detallados en secciones posteriores: la descripción de la serie, la adecuación de un modelo o técnica estocástica, y el pronóstico para hasta un horizonte $h$ determinado; el análisis de la serie debe preguntarse sobre el tipo de serie que se está analizando, el tipo de datos y el periodo de referencia utilizado para ajustar el modelo que servirá para realizar los pronósticos.

Existen múltiples formas de proceder mediante la etapa de estimación, como lo son los métodos de suavizamiento exponencial [@brown], modelos de regresión para series temporales [@kedem], redes neuronales secuenciales aplicadas a datos longitudinales [@redes], estimaciones bayesianas [@bayes], y finalmente, los procesos Autorregresivos Integrados de Medias Móviles o ARIMA por sus siglas en inglés [@box-jenkins], siendo estos últimos el foco de interés en este estudio. Los modelos ARIMA se enfocan en considerar las relaciones pasadas de un serie cronológica asociando los datos de las correlaciones totales y parciales. La forma de abordar una serie de tiempo utilizando los modelos ARIMA consiste, de forma muy general, en hacer una descripción de la serie para corroborar que se trate de una serie estacionaria y, de no serlo, someterla a procesos matemáticos para asegurar esta condición. Posteriormente, se realiza una identificación del posible proceso que gobierna la serie cronológica para luego estimar el modelo del orden seleccionado, sometiendo este a diversas pruebas de bondad de ajuste y rendimiento para finalmente verificar la calidad de los pronósticos obtenidos. El sustento teórico de cada una de estas será discutido a lo largo de este capítulo, que se compone de seis apartados. El primer apartado abarca los cuatro componentes de una serie cronológica. La segunda sección repasa los supuestos fundamentales en el análisis de series cronológicas. Con los elementos más básicos introducidos, el tercer apartado cubre el eje central de esta investigación: Los modelos Autorregresivos Integrados de Medias Móviles y sus componentes, los modelos autorregresivos y los modelos de medias móviles, así como la metodología Box-Jenkins y el proceso para la identificación de los modelos. En el cuarto apartado se introducen los métodos para la identificación de los modelos. El quinto apartado abarca los componentes relacionados a los autocorrelogramas, la forma más difundida para la selección de modelos y, finalmente, el sexto apartado introduce el principal aporte de este estudio, la sobreparametrización como método selección de casos.

\subsection{Componentes de una serie cronológica}

En el análisis de series cronológicas existen dos grandes corrientes de estudio: Los componentes inherentes a la serie cronológica y el estudio de las autocorrelaciones. Según el primer enfoque, de acuerdo con @oscarh-1, las series cronológicas poseen tres componentes principales: Tendencia-ciclos, Estacionalidad e Irregularidad. Considerando estos tres elementos, las series cronológicas pueden ser *aditivas*, como se muestra en la ecuación \ref{eqn:serie_aditiva}, en cuyo caso se asume que los tres componentes son independientes entre sí; o *multiplicativa*, donde, por el contrario, los tres componentes no son independientes, como muestra la ecuación \ref{eqn:serie_multiplicativa}.

\begin{equation}
\label{eqn:serie_aditiva}
Y(t)=T(t)+S(t)+I(t)
\end{equation}

\begin{equation}
\label{eqn:serie_multiplicativa}
Y(t)=T(t)\times S(t)\times I(t)
\end{equation}

Donde $Y$ es la serie cronológica, $T$ es la tendencia-ciclo, $S$ es la parte estacional, $I$ la parte irregular o aleatoria, y $t$ es el momento en el tiempo. Esta perspectiva clásica del análisis de series de tiempo permite realizar un análisis descriptivo del comportamiento de la serie en cuestión; cada una de sus partes se definen en posteriores apartados. De manera visual, una serie cronológica aditiva posee un comportamiento similar al mostrado en la Figura \ref{fig:ejemplo_aditiva}, mientras que un comportamiento multiplicativo puede apreciarse en la Figura \ref{fig:ejemplo_multiplicativa}.

```{r ejemplo_aditiva, fig.cap="Número de matrimonios en Costa Rica para el periodo 1978-1983"}
library(dplyr)
library(ggplot2)
library(ggpmisc)

TS <- stats::ts(c(2522,2750,2210,3154,2392,2288,2800,2178,2316,1782,2068,3436,2344,2980,3334,2184,2212,2838,2560,2380,2254,1956,2270,3410,2264,3144,3400,2260,2924,2440,2550,2788,2114,2068,2516,3488,2320,2858,2716,2340,2638,2374,2550,2746,2160,2388,2194,3010,2504,2898,3124,2544,3010,2494,3330,2494,2278,2446,2436,3676,2630,2928,3218,3012,2794,2778,3322,2734,2600,2734,2664,3546), 
         start = c(1978,1), end = c(1983, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Elaboración propia a partir de datos de la Unidad de Estadísticas Demográficas - INEC Costa Rica.",
       y="Matrimonios",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))# +
  #scale_x_discrete(expand = c(0,0), limits=c(1995, 2005, 2015))
```

```{r ejemplo_multiplicativa, fig.cap="Número de turistas en Costa Rica para el periodo 1991-2000"}
TS <- ts(c(47204, 45584, 48206, 36619, 31607, 36859, 47855, 45846, 33029, 37674, 43416, 50750, 58361, 58168, 54148, 45539, 41879, 46124, 57957, 52587, 36224, 40615, 51978, 67011, 70545, 69626, 63736, 53469, 43217, 47328, 66448, 55842, 42217, 45353, 56181, 70043, 83163, 73510, 78045, 57318, 48028, 50142, 67431, 64030, 49012, 52665, 61670, 76434, 88908, 76361, 72734, 60732, 52293, 54564, 70297, 61089, 49291, 51141, 68329, 78871, 90627, 80543, 78923, 60261, 50696, 57057, 66121, 60594, 44720, 48271, 62856, 80458, 91584, 80709, 77573, 58597, 54849, 60822, 74928, 62568, 50868, 54933, 62692, 81367, 101145, 89743, 89327, 78634, 64476, 71379, 85030, 72376, 56949, 64173, 72802, 96819, 117108, 98694, 102553, 81663, 69663, 76924, 92211, 80765, 59367, 66001, 81174, 105462, 115990, 106290, 107929, 87931, 75436, 77011, 91906, 78326, 65258, 68832, 93995, 119171), 
         start = c(1991,1), end = c(2000, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Elaboración propia a partir de datos de la Unidad de Estadísticas Demográficas - INEC Costa Rica.",
       y="Turistas",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10))) +
  scale_x_discrete(expand = c(0,0), limits=c(1991:2000))
```

\subsubsection{La tendencia-ciclo}

A partir del texto de @calderon2012estadistica, la tendencia general de una serie cronológica se refiere al crecimiento, decrecimiento o lateralización de sus movimientos a lo largo del periodo de estudio. La descomposición clásica de la tendencia-ciclo de este componente se mantiene constante de un periodo al siguiente y se obtiene a partir de una media móvil de $m$ periodos desde el momento $t$ $\left(\bar y_{t,m}\right)$. De esta manera la forma matemática de la tendencia-ciclo para una serie cronológica se muestra en la ecuación \ref{eqn:descomposicion_tendencia}.

\begin{equation}
\label{eqn:descomposicion_tendencia}
T(t)=
\begin{cases}
2\bar y_{t,m}, & \text{si}\ \text{m es par} \\
\bar y_{t, m}, & \text{si}\ \text{m es impar} \\
\end{cases}
\end{equation}

Un ejemplo es la serie cronológica del número de matrimonios en Costa Rica para el periodo 1978-1983, que con el tiempo su crecimiento suele comportarse de una forma creciente tal y como muestra la Figura \ref{fig:ejemplo_tendencia}. 

```{r ejemplo_tendencia, fig.cap="Tendencia del número de matrimonios en Costa Rica para el periodo 1978-1983"}
TS <- ts(c(2522,2750,2210,3154,2392,2288,2800,2178,2316,1782,2068,3436,2344,2980,3334,2184,2212,2838,2560,2380,2254,1956,2270,3410,2264,3144,3400,2260,2924,2440,2550,2788,2114,2068,2516,3488,2320,2858,2716,2340,2638,2374,2550,2746,2160,2388,2194,3010,2504,2898,3124,2544,3010,2494,3330,2494,2278,2446,2436,3676,2630,2928,3218,3012,2794,2778,3322,2734,2600,2734,2664,3546), 
         start = c(1978,1), end = c(1983, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  stat_smooth(color = "#FC4E07", fill = "#FC4E07",
              method = "lm")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Elaboración propia a partir de datos de la Unidad de Estadísticas Demográficas - INEC Costa Rica.",
       y="Matrimonios",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))
```

Del informe elaborado también por @calderon2012estadistica se desprende que los periodos cíclicos, por su parte, se refieren a los cambios que se dan en una serie cronológica en el mediano-largo plazo, que son causados por determinados eventos que suelen repetirse. Estos ciclos suelen tener una duración determinada, como es el caso de los índice bursátil NASDAQ-100. Este indicador resume el estado de los 100 valores de las compañías más importantes del sector de la industria de la tecnología, y sus ciclos suelen presentar un auge, seguido por un descenso que, posteriormente, se vuelve una depresión, y que finalmente se convierte en una recuperación a su estado inicial. La Figura \ref{fig:ejemplo_ciclo} muestra como el índice NASDAQ-100 inicia un auge alrededor de enero de 1995 (primera línea azul punteada), para luego experimentar una fuerte caída a partir de junio del año 2000 (línea roja punteada) y posteriormente iniciar un periodo de recuperación en enero del año 2009 (segunda línea azul punteada).

```{r ejemplo_ciclo, fig.cap="Índice bursatil NASDAQ-100 para el periodo enero 1990 - junio 2021", message=FALSE}
ggplot(log(ts(readr::read_csv(read_from_dir("^NDX.csv", "Data/Raw"))$Close, 
   start = c(1990, 1), end = c(2021, 6), frequency = 12))) +
  geom_line(color = "#00AFBB", size = 1.3) +
  geom_vline(xintercept = c(1995.1, 2009.1, 2000.6), linetype="dashed", color = c("blue", "blue", "red"))+
   theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Elaboración propia a partir de datos de Yahoo Finance.",
       y="Precios de cierre (escala logarítmica)",
       x="Periodo") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))
```

\subsubsection{Componentes estacionales}

@calderon2012estadistica también se refiere a los cambios estacionales que se presentan en una serie de tiempo, los cuales se relacionan con las fluctuaciones naturales del fenómeno dentro de una temporada de observaciones. Visualmente los efectos estacionales puede apreciarse en la Figura \ref{fig:ejemplo_multiplicativa}, en donde los picos más altos de turistas siempre se ubican entres los meses de diciembre y enero. Matemáticamente en su forma clásica, el componente estacional puede calcularse como se indica en la ecuación \ref{eqn:descomposicion_estacional}:

\begin{equation}
\label{eqn:descomposicion_estacional}
S(t)=\psi_{t,a}-\psi^*
\begin{cases}
\psi_{t,a} = \frac{\sum_{i=0}^a \hat{y}_{t+a\cdot i}}{a} \\
\psi^*=\frac{\sum_{i=0}^n \hat{y}_{t+i}}{n} \\
\hat{y}_t=y_t-\bar{y}_{t,S,m} \\
\bar{y}_{t,S,m}=\frac{\sum_{i=1}^m \bar{y}_{t+i-m,S}}{m} \\
\bar{y}_{t,S}=\frac{\sum_{i=1}^S y_{t-i+\frac{S}{2}}}{S} \\
\end{cases}
\end{equation}

donde $S$ representa la frecuencia estacional (por ejemplo cada 12 meses), $a$ es la cantidad de periodos estacionales disponibles (por ejemplo, si se tienen 6 periodios completos de 12 meses, se tienen 6 años) y $m$ es la cantidad de periodos que se utiliza para centrar las medias móviles. Gráficamente, el componente estacional se muestra en la Figura \ref{fig:ejemplo_estacional}.

```{r ejemplo_estacional, fig.cap="Componente aleatorio de la serie de matrimonios en Costa Rica para el periodo 1978-1983"}
TS <- ts(c(2522,2750,2210,3154,2392,2288,2800,2178,2316,1782,2068,3436,2344,2980,3334,2184,2212,2838,2560,2380,2254,1956,2270,3410,2264,3144,3400,2260,2924,2440,2550,2788,2114,2068,2516,3488,2320,2858,2716,2340,2638,2374,2550,2746,2160,2388,2194,3010,2504,2898,3124,2544,3010,2494,3330,2494,2278,2446,2436,3676,2630,2928,3218,3012,2794,2778,3322,2734,2600,2734,2664,3546), 
         start = c(1978,1), end = c(1983, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(decompose(TS)$seasonal)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Elaboración propia a partir de datos de la Unidad de Estadísticas Demográficas - INEC Costa Rica.",
       y="Magnitud del componente estacional",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))
```

\subsubsection{Componente irregular}

Finalmente, la irregularidad de una serie cronológica, siguiendo a @calderon2012estadistica, se refiere a las fluctuaciones propias de un fenómeno que no pueden ser predichas. Estos cambios no se dan de manera regular, es decir, no siguen un patrón determinado. Matemáticamente su descomposición se obtiene a partir de los otros componentes así como de la propia serie cronológica $y(t)$, tal y como se muestra en la ecuación \ref{eqn:descomposicion_irregular}. Visualmente, la magnitud del componente aleatorio se muestra en la Figura \ref{fig:ejemplo_aleatorio}

\begin{equation}
\label{eqn:descomposicion_irregular}
I(t)=
\begin{cases}
y(t)-T(t)-S(t), & \text{si}\ \text{la serie es aditiva} \\
\frac{y(t)}{T(t)S(t)} , & \text{si}\ \text{la serie es multiplicativa} \\
\end{cases}
\end{equation}

```{r ejemplo_aleatorio, fig.cap="Componente aleatorio de la serie de matrimonios en Costa Rica para el periodo 1978-1983"}
TS <- ts(c(2522,2750,2210,3154,2392,2288,2800,2178,2316,1782,2068,3436,2344,2980,3334,2184,2212,2838,2560,2380,2254,1956,2270,3410,2264,3144,3400,2260,2924,2440,2550,2788,2114,2068,2516,3488,2320,2858,2716,2340,2638,2374,2550,2746,2160,2388,2194,3010,2504,2898,3124,2544,3010,2494,3330,2494,2278,2446,2436,3676,2630,2928,3218,3012,2794,2778,3322,2734,2600,2734,2664,3546), 
         start = c(1978,1), end = c(1983, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(decompose(TS)$random)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Elaboración propia a partir de datos de la Unidad de Estadísticas Demográficas - INEC Costa Rica.",
       y="Magnitud del componente aleatorio",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))
```

\subsection{Supuestos en el análisis de series cronológicas}

```{r, eval=FALSE, echo=FALSE}
#PROCESO ESTOCASTICO: http://www.ccs.fau.edu/~bressler/EDU/STSA/Modules/I.pdf
```

El análisis de series temporales, según @Hipel, representa un método para comprender la naturaleza de la serie en cuestión y poder utilizarla para generar pronósticos. Es en este sentido que entran en escena las observaciones recolectadas de la serie, pues ellas son analizadas y sujetas a modelados matemáticos que logren capturar el proceso que gobierna a toda la serie cronológica [@Zhang].

En un proceso determinístico, es posible predecir con certeza lo que ocurrirá en el futuro pues carecen de aleatoriedad, razón por la cual el proceso definirse fácilmente mediante una ecuación matemática; las series cronológicas, sin embargo, carecen de esta condición. Por el contrario, los procesos no determinísticos son aquellos que no pueden describirse con exactitud mediante una ecuación matemática, sino que deben aproximarse debido al componente aleatorio que poseen de forma intrínseca. Una serie de tiempo puede tratarse de un proceso no determinístico porque usualmente, toda la información que se necesita para describir el proceso de manera determinística es desconocida, o bien porque la naturaleza de la información involucra la aleatoriedad, de esta manera, como los procesos no determinísticos consideran un aspecto aleatorio, pueden estimarse mediante leyes probabilísticas. @Hipel sugieren que una serie cronológica puede considerarse como una muestra aleatoria de una serie mucho más grande, es decir, una serie cronológica puede verse como una colección de variables aleatorias ordenadas de manera cronológica. La magnitud de estas variaciones aleatorias pueden variar en función del tiempo, esta condición se conoce como un proceso estocástico (aleatorio) [@definicion_estocastico]. 

De acuerdo con @stationary_def, una serie se considera estacionaria cuando su nivel medio y su variancia son aproximadamente las mismas durante todo el periodo, es decir, el tiempo no afecta a estos estadísticos de variabilidad. A partir del texto de @introduccion_series, una forma de definir un proceso estacionario $Y_t$ es mediante los momentos poblacionales de primer y segundo orden tal y como se define en la ecuación \ref{eqn:ecuacion_estocastico}.

```{r, eval=FALSE, echo=FALSE}
# Se encontró en el siguiente enlace:

#https://books.google.es/books?id=KvLhxFPwvsUC&pg=PA13&dq=un+proceso+estoc%C3%A1stico+es&hl=es&sa=X&redir_esc=y#v=onepage&q=un%20proceso%20estoc%C3%A1stico%20es&f=false
```

\begin{equation}
\label{eqn:ecuacion_estocastico}
Y_t:
\begin{cases}
E(Y_t) = \mu_t, \forall t \\
V(Y_t) = \sigma^2_t, \forall t \\
COV(T_t,Y_s) = E\left[(Y_t-\mu_t)(Y_s-\mu_s)\right], \forall t,s \\
\end{cases}
\end{equation}

El supuesto de estacionariedad busca simplificar la identificación del proceso con el objetivo de obtener un modelo adecuado para generar los pronósticos. De acuerdo con [@definicion_estocastico], se dice que una serie cronológica $Y_t$ es fuertemente estacionaria cuando la distribución de probabilidad de dicho proceso en cualquier momento $t$ es aproximadamente la misma para cualquier momento en el tiempo, mientras que es débilmente estacionaria si su media y su función de correlación no varía en el tiempo. Si una serie cronológica posee tendencias o patrones estacionales hace que esta sea no estacionaria. En la práctica, una serie puede volverse estacionaria al aplicarle transformaciones o diferenciaciones de distinto orden.

```{r, eval=FALSE, echo=FALSE}
# Ver página 121
# https://books.google.co.cr/books?id=ZU3MEKZFgsMC&pg=PA121&dq=independiente+e+id%C3%A9nticamente+distribu%C3%ADda&hl=es-419&sa=X&ved=2ahUKEwiH8cjA2b_xAhV2nWoFHcE6BdQQ6AEwAHoECAYQAg#v=onepage&q=independiente%20e%20id%C3%A9nticamente%20distribu%C3%ADda&f=false
```

Como una serie temporal es una observación o una realización de un proceso estocástico, éstas se encuentran sujetas a múltiples supuestos. Uno de ellos es que todas las observaciones son independientes e idénticamente distribuidas (i.i.d.), que según @definicion_iid, un conjunto de variables aleatorias $Y_1,...,Y_n$ son independientes e idénticamente distribuidas si el conjunto es independiente y además cada una de las n variables sigue la misma distribución, que usualmente se define como una distribución aproximadamente Normal, con una media y variancia dadas; esta condición es deseable, pues un proceso estocástico puede no ser independiente al tener una estructura que genere un patrón reiterado en el tiempo. Este supuesto puede dividirse según el tipo de variable aleatoria:

**1.**: Si el conjunto de variables $Y_1,...,Y_n$ pertenecen a una distribución discreta, cada función de probabilidad es idéntica, de manera que $p_{y_1}(y)=p_{y_2}(y)=\cdots=p_{y_n}(y)\equiv p(y)$, y además $p_{y_1,...,y_n}(y_1,...,y_n)=p_{y_1}(y_1)p_{y_2}(y_2)\cdots p_{y_n}(y_n)=p(y_1)p(y_2)\cdots p(y_n)$.

**2.**: Si el conjunto de variables $Y_1,...,Y_n$ pertenecen a una distribución continua, cada función de probabilidad es idéntica, de manera que $f_{y_1}(y)=f_{y_2}(y)=\cdots=f_{y_n}(y)\equiv f(y)$, y además $f_{y_1,...,y_n}(y_1,...,y_n)=f_{y_1}(y_1)f_{y_2}(y_2)\cdots f_{y_n}(y_n)=f(y_1)f(y_2)\cdots f(y_n)$.

Lo anterior es contrario al uso de las observaciones pasadas para pronosticar el futuro, por lo que este supuesto, según @Cochrane, no es exacto pues una una serie de tiempo no es exactamente, i.i.d., sino que siguen un patrón medianamente regular en el largo plazo.

El último supuesto, y quizá el que más debate genera, es el criterio de parsimonia. Como mencionan @Zhang y @Hipel, este principio sugiere que se prioricen modelos sencillos, con pocos parámetros, para representar una serie de datos. Mientras más grande y complicado sea el modelo, mayor será el riesgo de sobre ajuste, lo que implica que el ajuste sea muy bueno en el conjunto de datos con que se generó el modelo, pero que los pronósticos generados sean pobres ante nuevos conjuntos de datos. Este problema, sin embargo, se presenta al considerar un único modelo con muchos parámetros; pero si se consideran varios modelos y estos son sometidos a distintos criterios, puede obtenerse un modelo sobreparametrizado que ofrezca buenos pronósticos.

\subsection{Identificación del modelo}

Los métodos más clásicos para la identificación del proceso que gobierna a una serie cronológica son las funciones de autocorrelación y autocorrelación parcial, las cuales sirven de indicador acerca de qué tan relacionadas están las observaciones unas de otras. Estas funciones ofrecen indicios sobre el orden de los términos para los modelos $AR(p)$, $MA(q)$ y para la diferenciación y, por ende, para la identificación de un modelo $ARIMA$ [@hyndman_box-jenkins].

Para medir la relación lineal entre dos variables cuantitativas es común utilizar el coeficiente de correlación $r$ de Pearson [@pearson], el cual se define para dos variables $X$ e $Y$ como se muestra en la ecuación \ref{eqn:pearson}.

\begin{equation}
\label{eqn:pearson}
r_{X,Y}=\frac{E(XY)}{\sigma_X \sigma_Y} = \frac{\sum_{i=1}^n \left(X_i- \bar X\right) \left(Y_i- \bar Y\right)}{\sqrt{\sum_{i=1}^n \left(X_i- \bar X\right)^2 \sum_{i=1}^n \left(Y_i- \bar Y\right)^2}}
\end{equation}

Este mismo concepto puede aplicarse a las series cronológicas para comparar el valor de la misma en el tiempo $t$, con su valor en el tiempo $t-1$, es decir, se comparan las observaciones consecutivas $Y_t$ con $Y_{t-1}$. Esto también es aplicable a no solo una observación rezagada $(Y_{t-1})$, sino también con múltiples rezagos $(Y_{t-2}), (Y_{t-3}), \cdots,(Y_{t-n})$. Para esto se hace uso del coeficiente de autocorrelación.

La función de autocorrelación (*ACF* por sus siglas en inglés) recibe su nombre debido a que se utiliza el coeficiente de correlación para pares de observaciones $r_{Y_t, Y_{t-1}}$ de la serie cronológica. Al conjunto de todas las autocorrelaciones se le llama función de autocorrelación.

La función de autocorrelación parcial[^5], busca medir la asociación lineal entre las observaciones $Y_t$ y $Y_{t-k}$, es decir, la correlación entres dos observaciones distintas separadas por $k$ periodos, descartando los efectos de los rezagos $1,2, \cdots ,k-1$; esta correlación puede obtenerse a partir de la ecuación \ref{eqn:pearson}, que al adaptarse a dos observaciones de la misma serie cronológica se obtiene el resultado de ecuación \ref{eqn:pearson_adaptada_autocorrelacion}.

\begin{equation}
\label{eqn:pearson_adaptada_autocorrelacion}
r_{{Y_t},{Y_{t-k}}}=\frac{E({Y_t}{Y_{t-k}})}{\sigma_{Y_t} \sigma_{Y_{t-k}}} = \frac{\sum_{i=1}^n \left({Y_t}_i- \bar{Y}_t \right) \left({Y_{t-k}}_i- \bar {Y_{t-k}}\right)}{\sqrt{\sum_{i=1}^n \left({Y_t}_i- \bar{Y}_t \right)^2 \sum_{i=1}^n \left({Y_{t-k}}_i- \bar {Y_{t-k}}\right)^2}}
\end{equation}

De lo anterior se deduce entonces que un las k observaciones previas pueden utilizarse para obtener el valor de de la serie cronológica en el momento $t$, como muestra la ecuación \ref{eqn:ecuacion_autocorrelaciones}.

\begin{equation}
\label{eqn:ecuacion_autocorrelaciones}
y_t=\phi_{k1}y_{t-1}+\phi_{k2}y_{t-2}+\cdots+\phi_{kk}y_{t-k}+u_t,k=1,2,...,K
\end{equation}

Los valores de cada término $\phi_{kk}$, asumiendo que pertenecen a un proceso estacionario, suelen estimarse mediante la ecuación de Yule-Walker [@yule.walker], cuya forma más general se muestra en la ecuación \ref{eqn:ecuacion_yule_walker_general}.

\begin{equation}
\label{eqn:ecuacion_yule_walker_general}
\gamma_i=E\left[ \phi_{k1}y_{t-1}y_{t-i} + \phi_{k2}y_{t-2}y_{t-i} + \cdots + \phi_{kn}y_{t-n}y_{t-i} + u_ty_{t-i} \right]=\phi_{k1}\gamma_{i-1} + \phi_{k2}\gamma_{i-2} + \cdots + \phi_{kn}\gamma_{n-i}
\end{equation}

Al considerar la ecuación en términos de la función de autocorrelación se obtiene lo siguiente:

\begin{equation}
\label{eqn:ecuacion_yule_walker_autocorrelacion}
\rho_i=\phi_{k1}\rho_{i-1} + \phi_{k2}\rho_{i-2} + \cdots + \phi_{kn}\rho_{n-i}+\cdots
\end{equation}

Alternando los distintos valores de $k$ a partir de la ecuación \ref{eqn:ecuacion_yule_walker_autocorrelacion}, se obtiene el sistema de ecuaciones mostrado en \ref{eqn:sistema_yule_walker}.

\begin{equation}
\label{eqn:sistema_yule_walker}
\begin{array}{lll} 
\rho_1=\phi_{k1} + \phi_{k2}\rho_{1} + \cdots + \phi_{kn}\rho_{n-1}+\cdots \\
\rho_2=\phi_{k1}\rho_{1} + \phi_{k2} + \cdots + \phi_{kn}\rho_{n-2}+\cdots \\
\rho_3=\phi_{k1}\rho_{2} + \phi_{k2}\rho_{1} + \cdots + \phi_{kn}\rho_{n-3}+\cdots \\
\vdots \\
\rho_k=\phi_{k1}\rho_{k-1} + \phi_{k2}\rho_{k-2} + \cdots + \phi_{kn}\rho_{n-k}+\cdots
\end{array}
\end{equation}

Como resultado del sistema de ecuaciones mostrado en \ref{eqn:sistema_yule_walker}, es posible hacer un replanteamiento en forma de un sistema matricial del cuál las autocorelaciones parciales pueden obtenerse a partir del despeje del vector $\Phi$ en \ref{eqn:sistema_yule_walker_matricial}.

\begin{equation}
\label{eqn:sistema_yule_walker_matricial}
\left[ \begin{array}{c} \rho_1 \\ \rho_2 \\ \vdots \\ \rho_k \end{array} \right] = \begin{bmatrix} 1 & \rho_1 & \rho_2 & \cdots & \rho_{k-1} \\ \rho_1 & 1 & \rho_1 & \cdots & \rho_{k-2} \\ \vdots & \vdots & \vdots & \cdots & \vdots \\ \rho_{k-1} & \rho_{k-2} & \rho_{k-3} & \cdots & 1 &\end{bmatrix} \left[ \begin{array}{c} \phi_{k1} \\ \phi_{k2}  \\ \vdots \\ \phi_{kk} \end{array} \right]\end{equation}

[^5]: *PACF* por sus siglas en inglés

Cuando se tiene el modelo ARIMA debidamente identificado, es importante realizar los pronósticos. Sin embargo, estos pronósticos no son imperativos, sino que se debe evaluar su calidad con las llamadas medidas de rendimiento. Estas mediciones son hechas comparando el pronóstico y su diferencia con el valor real. Existen múltiples medidas de rendimiento, @medidas menciona entre ellas el *MAE*, *MAPE*, *RMSE*, *MASE*, *AIC*, *AICc* y el *BIC*.

\subsection{Modelos Autorregresivos Integrados de Medias Móviles}

Hay dos grandes grupos de modelos lineales de series cronológicas: Los modelos Autorregresivos (AR) [@Lee] y los modelos de Medias Móviles (MA) [@box-jenkins]. La combinación de estos dos grandes grupos forman los Modelos Autorregresivos de Medias Móviles (ARMA) [@Hipel] y los modelos Autorregresivos Integrados de Medias Móviles (ARIMA), siendo este último de particular interés en esta investigación.

Los modelos ARIMA son los de uso más extendido en el análisis de series cronológicas. Se fundamentan en las autocorrelaciones pasadas, y contempla un proceso iterativo para identificar un posible proceso óptimo a partir de una clase general de modelos. El teorema de Wold [@Wold] sugiere que todo proceso estacionario puede ser determinado de una forma específica y cuya ecuación posee, en realidad, infinitos coeficientes, pero que debe ser reducido a una cantidad finita para luego evaluar su ajuste sometiéndolo a diferentes pruebas y medidas de rendimiento.

\subsubsection{Ecuación de Wold}

Según @sargent_macro, cualquier proceso estacionario puede ser representado mediante la ecuación \ref{eqn:proceso_estacionario}:

\begin{equation}
\label{eqn:proceso_estacionario}
x_t=\sum_{j=0}^{\infty} \psi_j\varepsilon_{t-j}+\kappa_t
\end{equation}

donde $\forall \psi_j \in \mathbb{R}, \psi_0=1, \sum_{j=0}^{\infty} \psi_j^2<\infty$, y $\varepsilon_t$ representa un ruido blanco gaussiano i.i.d., es decir, $\varepsilon_t \sim N(0, \sigma^2)$; además, $\kappa_t$ es el componente lineal determinístico de forma tal que $cov(\kappa_t,\varepsilon_{t-j}=0)$, lo cual implica que este componente determinístico es independiente de la suma infinita de los choques pasados.

De lo anterior, si se omite la parte determinística $\kappa_t$ de \ref{eqn:proceso_estacionario}, el remanente es la suma ponderada infinita, lo cual implica que si se conocen los ponderadores $\psi_j$, y si además se conoce $\sigma_\varepsilon^2$, es posible obtener una representación para cualquier proceso estacionario; este concepto es conocido como *media móvil infinita*.

Sabiendo que $\varepsilon_t \sim N(0, \sigma^2)$, se tiene que $\varepsilon_t$ tiene media 0, es decir, está centrado en este valor. De esta manera el ruido blanco es por definición un proceso centrado, lo cual implica que la suma ponderada infinita está centrada en sí misma. De esta manera, la representación de Wold de un proceso $x_t$ supone que se suman los choques pasados más un componente determinístico que no es otro que el valor esperado del proceso: $\kappa_t=m$, donde $m$ es una constante cualquiera. Así, la ecuación \ref{eqn:proceso_estacionario} puede sustuirse por:

\begin{equation}
\label{eqn:proceso_estacionario2}
x_t=\sum_{j=0}^{\infty} \psi_j\varepsilon_{t-j}+m
\end{equation}

y de \ref{eqn:proceso_estacionario2} puede verificarse que,

\begin{equation}
\label{eqn:dem_proceso_estacionario2}
E(x_t)=E\left(\sum_{j=0}^{\infty} \psi_j\varepsilon_{t-j}+m\right)=\sum_{j=0}^{\infty} \psi_jE\left(\varepsilon_{t-j}\right) + m = m
\end{equation}

La principal consecuencia del teorema de Wold es que, si se conocen los ponderadores $\psi_j$, y además $\sigma_\varepsilon^2$ es ruido blanco es posible conocer el proceso por medio del cual se rige la serie cronológica. Esto permite realizar cualquier previsión, denotada por $\hat X_{T+h}$ para el proceso de interés $x_T$ en el momento $T+h$ para una muestra cualquiera de $T$ observaciones de $x_t$. De acuerdo con @sargent_macro, basado en el teorema de Wold, la mejor previsión posible para un proceso $x_t$ para el momento $T+h$, denotado por $\hat x_{T+h}$, la predicción está dada por:

\begin{equation}
\label{eqn:prevision}
\hat x_{T+h}=\sum_{j=1}^{\infty} \psi_j \varepsilon_{T-j+1}
\end{equation}

De la ecuación \ref{eqn:prevision} se desprende que el error de previsión asociado está dado por:

\begin{equation}
\label{eqn:error_prevision}
x_{T+h}- \hat x_{T+h}=\sum_{j=1}^{\infty} \psi_j \varepsilon_{T-h+1}
\end{equation}

De esta manera, la ecuación de Wold se convierte en una representación base para representar a una serie cronológica que está gobernada por un determinado proceso, y que al no ser conocido, resulta necesario contar con una herramienta para su aproximación.

\subsubsection{Metodología Box-Jenkins}

La combinación de un $AR(p)$ y un $MA(q)$, descritos en las ecuaciones \ref{eqn:modelo_AR} y \ref{eqn:modelo_MA} respectivamente, como se mencionó al inicio de esta sección, generan los modelos autorregresivos de medias móviles, $ARMA(p,q)$, representados mediante la ecuación \ref{eqn:modelo_ARMA}.

\begin{equation}
\label{eqn:modelo_ARMA}
y_t=c+\varepsilon_t+\sum_{i=1}^p \varphi_iy_{t-i}+\sum_{j=1}^q \theta_j \varepsilon_{t-j}
\end{equation}

@Cochrane menciona que los modelos $ARMA(p,q)$ suelen manipularse mediante lo que se conoce como operador de rezagos, denotado como $Ly_t=y_{t-1}$. Esto significa que en un $AR(p)$ se tiene que $\varepsilon_t=\varphi(L)y_t$, mientras que en $MA(q)$ se tiene que $y_t=\theta(L)\varepsilon_t$, y por consiguiente en un $ARMA(p,q)$ se tiene $\varphi(L)y_t=\theta(L)\varepsilon_t$. Por lo tanto, de lo anterior se desprende que $\varphi(L)=1-\sum_{i=1}^p \varphi_iL^i$, y que $\theta(L)=1+\sum_{j=1}^q\theta_jL^j$.

Los modelos $ARMA$, sin embargo, solamente pueden ser utilizados en series cronológicas cuyo proceso es estacionario. Esto, en la práctica, es poco común, pues una serie de tiempo a menudo posee tendencias y ciertos patrones estacionales y, además, como menciona @Hamzacebi, presentan procesos no estacionarios por naturaleza. Esta condición hace necesaria la introducción de una generalización de los modelos $ARMA$, la cual se conoce como los modelos $ARIMA$ [@box-jenkins].

\subsubsection{Modelos Autorregresivos}

Un modelo autorregresivo de orden *p*, denotado como $AR(p)$, considera los valores futuros de una serie cronológica como una combinación lineal las $p$ observaciones predecesoras, un componente aleatorio y un término constante. @Hipel y @Lee emplean la notación de la ecuación \ref{eqn:modelo_AR}.

\begin{equation}
\label{eqn:modelo_AR}
y_t=c+\sum_{i=1}^p \varphi_iy_{t-i}+\varepsilon_t
\end{equation}

Donde $y_t$ y $\varepsilon_t$ corresponden al valor de la serie y al componente aleatorio en el momento actual $t$, mientras que $\varphi_i$, con $i=1,2,\cdots,p$ son los parámetros del modelo, y $c$ es su término constante, que en ciertas ocasiones se suele omitir para simplificar la notación. 

\subsubsection{Modelos de Medias Móviles}

De manera similar a como un $AR(p)$ utiliza los valores pasados para pronosticar los futuros, los modelos de medias móviles de orden q, denotados como $MA(q)$, utilizan los errores pasados de las variables independientes. Estos modelos se describen mediante la ecuación \ref{eqn:modelo_MA}.

\begin{equation}
\label{eqn:modelo_MA}
y_t=\mu+\sum_{j=1}^q \theta_j \varepsilon_{t-j}+\varepsilon_t
\end{equation}

Donde $\mu$ representa el valor medio de la serie cronológica y cada valor de $\theta_j(j=1,2,\cdots,q)$ son los parámetros del modelo. Como los $MA(q)$ utilizan los errores pasados de la serie cronológica, se asume que estos son i.i.d. centrados en cero y con una variancia constante, siguiendo una distribución aproximadamente Normal, con lo cual este tipo de modelos pueden considerarse como una regresión lineal entre una observación determinada y los términos de error que le preceden [@stationary_def]. 

\subsubsection{Modelos ARIMA}

Partiendo de una serie con un proceso no estacionario, es posible aplicar transformaciones o diferenciaciones (*d*)a los datos con el objetivo de convertirlos en un proceso estacionario. Utilizar la notación de rezagos descrita anteriormente, según @Lombardo, permite plantear un modelo $ARIMA(p,d,q)$ como se describe en la ecuación \ref{eqn:modelo_ARIMApdq}.

\begin{equation}
\label{eqn:modelo_ARIMApdq}
\varphi(L)(1-L)^dy_t=\theta(L)\varepsilon_t\\
\left(1-\sum_{i=1}^p \varphi_iL^i \right)(1-L)^d y_t=\left(1+\sum_{j=1}^q \theta_jL^j \right) \varepsilon_t
\end{equation}

Donde los términos $p, d$ y $q$ son positivos y mayores a cero y corresponden al modelo autorregresivo, a la diferenciación y al modelo de medias móviles, respectivamente. El componente $d$ es el número de diferenciaciones, si $d=0$ se tiene un modelo ARMA, y $d\geq1$ representa el número de diferenciaciones; en la mayoría de casos $d=1$ suele ser suficiente. Así, un $ARIMA(p,0,0)=AR(p)$, $ARIMA(0,0,q)=MA(q)$, y un $ARIMA(0,1,0)=y_t=y_{t-1}+\varepsilon_t$, es decir, un modelo de caminata aleatoria.

Como sugieren @box-jenkins, lo anterior puede generalizarse aún más al considerar los efectos estacionales de la serie cronológica. Si se considera una serie cronológica con observaciones mensuales, una diferenciación de primer orden es igual a la diferencia entre una observación y la observación correpondiente al mismo mes pero del año anterior; es decir, si el periodo estacional es de $s=12$ meses, entonces esta diferencia estacional aplicada a un $ARIMA(p,d,q)(P,D,Q)_S$ es calculada mediante $z_t=y_t-y_{t-s}$. 

De esta manera, el método de @box-jenkins inicia con el análisis exploratorio de la serie cronológica, teniendo un interés particular en identificar si hay presencia de factores no estacionarios en la misma. Si en efecto se cuenta con una serie no estacionaria, ésta debe volverse estacionaria mediante algún tipo de transformación, típicamente el logaritmo natural. Con la serie ya transformada, se busca identificar el proceso que gobierna la serie. La forma clásica de hacer esto es mediante los gráficos de autocorrelación y autocorrelación parcial. Cuando se logra identificar un proceso que se adecue más a la serie cronológica, se deben realizar los diagnósticos para evaluar la calidad del ajuste del modelo, así como las medidas de rendimiento referentes a los pronósticos que genera el modelo estimado hasta un horizonte determinado.

\subsection{Los autocorrelogramas}

```{r, eval=FALSE}
#Lo siguiente se obtuvo del sitio:
#https://people.duke.edu/~rnau/arimrule.htm
```

El uso del *ACF* y el *PACF* se suele aplicar de manera visual. Sin embargo, hacer usos de estos elementos implica considerar múltiples condiciones. En el caso de la identificación del orden de la diferenciación:

 - Si la serie posee autocorrelaciones positivas en un amplio número de rezagos, entonces es posible que se requiera un orden más alto en el valor de $d$.
 - Si la autocorrelación en $t-1$ es menor o igual a cero, o si las autocorrelaciones resultan ser muy bajas y sin seguir algún patrón en particular, entonces no se requiere un alto orden para la diferenciación.
 - Una desviación estándar baja suele ser indicador de un orden adecuado de integración.
 - Si no se utiliza ninguna diferenciación, se asume que la serie cronológica es estacionaria. Aplicar una diferenciación asume que la serie cronológica posee una media constante, mientras que dos diferenciaciones sugiere que la tendencia varía en el tiempo.

Para la identificación de los términos $p$ y $q$:

 - Si la *PACF* de la serie cronológica diferenciada muestra una diferencia marcada y si, además, la autocorrelación en $t-1$ es positiva, entonces debe considerarse aumentar el valor de $p$.
 - Si la *PACF* de la serie cronológica diferenciada muestra una diferencia marcada y si, además, y la autocorrelación en $t-1$ es negativa, entonces debe considerarse aumentar el valor de $q$.
 - Los términos $p$ y $q$ pueden cancelar sus efectos entre sí, por lo que si se cuenta con un modelo $ARMA$ más mixto que parece adaptarse bien a los datos, puede deberse también a que $p$ o $q$ deben ser menores.
 - Si la suma de los coeficientes del modelo $AR$ es muy cercana a la unidad, es necesario reducir la cantidad de términos en uno y aumentar el orden de la diferenciación en uno.
 - Si la suma de los coeficientes del modelo $MA$ es muy cercana a la unidad, es necesario reducir la cantidad de términos en uno y disminuir el orden de la diferenciación en uno.

Para ejemplificar el uso de los autocorrelogramas en la identificación de modelos, se presenta en la Figura \ref{fig:ejemplo_ucr} la serie cronológica expuesta por @oscarh-1 de graduados de la Universidad de Costa Rica (UCR) para el periodo 1965-2002.

```{r ejemplo_ucr, fig.cap="Número anual de graduados de la Universidad de Costa Rica para el periodo 1965-2002"}
library(dplyr)
library(ggplot2)
library(ggpmisc)

TS <- ts(c(286,309,355,402,535,831,953,1142,1242,1539,1874,2244,2435,2626,2647,2643,2381,2532,2359,2626,2546,2804,2605,2975,3240,3449,3535,3785,3658,3963,3568,3779,4257,4161,4381,3955,4131,4118), start = 1965, end = 2002,
   frequency = 1)
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Introducción a las Series Cronológicas, Óscar Hernández.",
       y="Graduados",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))# +
  #scale_x_discrete(expand = c(0,0), limits=c(1995, 2005, 2015))
```

Tal y como menciona el autor, la serie cronológica posee una clara tendencia creciente a lo largo del tiempo, lo cual sugiere que no se trata de una serie estacionaria. Esto se confirma al analizar las funciones de autocorrelación simple y parcial de la serie cronológica en las Figuras \ref{fig:auto_ucr1} y \ref{fig:parcial_ucr1}; pues la función de autocorrelación no cae rápidamente a cero, sino que posee un descenso más pausado.

```{r auto_ucr1, fig.cap="Función de autocorrelación simple de la serie de graduados de la UCR"}
acf(TS, main="", xlab="Rezagos")
```

```{r parcial_ucr1, fig.cap="Función de autocorrelación parcial de la serie de graduados de la UCR"}
pacf(TS, main="", xlab="Rezagos")
```

Dado que la serie mostrada no es estacionaria, es posible aplicar una diferenciación para hacerla cumplir esta condición, tal y como se muestra en la Figura \ref{fig:ejemplo_ucr_diferenciada}. Al analizar la Figura \ref{fig:auto_ucr2} se observa cómo la función de autocorrelación cae rápidamente a cero, lo cual confirma que se posee una serie estacionaria. Posteriormente, para intentar identificar el proceso que gobierna la serie cronológica, puede verse que hay dos barras en la Figura \ref{fig:parcial_ucr2} y que además la función de autocorrelación de la Figura \ref{fig:auto_ucr2} cae rápidamente hacia cero, lo cual sugiere que se está en presencia de un modelo autorregresivo de orden 2.

```{r ejemplo_ucr_diferenciada, fig.cap="Serie diferenciada de graduados de la Universidad de Costa Rica para el periodo 1965-2002"}
library(dplyr)
library(ggplot2)
library(ggpmisc)

TS <- ts(c(286,309,355,402,535,831,953,1142,1242,1539,1874,2244,2435,2626,2647,2643,2381,2532,2359,2626,2546,2804,2605,2975,3240,3449,3535,3785,3658,3963,3568,3779,4257,4161,4381,3955,4131,4118), start = 1965, end = 2002,
   frequency = 1) %>% diff()
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Introducción a las Series Cronológicas, Óscar Hernández.",
       y="Graduados-diferenciada",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))# +
  #scale_x_discrete(expand = c(0,0), limits=c(1995, 2005, 2015))
```

```{r auto_ucr2, fig.cap="Función de autocorrelación simple de la serie diferenciada de graduados de la UCR"}
acf(TS, main="", xlab="Rezagos")
```

```{r parcial_ucr2, fig.cap="Función de autocorrelación parcial de la serie diferenciada de graduados de la UCR"}
pacf(TS, main="", xlab="Rezagos")
```

Tener en consideración estos y otros posibles criterios para la identificación del proceso que gobierna la serie cronológica puede fácilmente volverse subjetivo, pues dos personas diferentes pueden llegar a dar distintas interpretaciones a las visualizaciones de los autocorrelogramas. Estas interpretaciones pueden sesgar la identificación de los modelos y, además, no considerar otros escenarios para los términos de un modelo $ARIMA$; para solventar esto es necesario considerar un abanico más amplio de opciones que a su vez elimine el criterio subjetivo del observador, lo cual se puede lograr al considerar múltiples permutaciones de términos para contrastar una gran cantidad de modelos, es decir, utilizar la sobreparametrización.

\subsection{La sobreparametrización y el análisis combinatorio}

La identificación visual mediante los autocorrelogramas puede llevar a decisiones erradas acerca del proceso que gobierna la serie cronológica. Una alternativa es considerar estimaciones procesos de ordenes bajos, como un $ARMA(1,1)$ y poco a poco ir incorporando términos, este proceso de revisión permite encontrar los puntos en que agregar un coeficiente más al modelo no aporta ninguna mejora en los resultados del pronóstico, y así considerar únicamente aquellos modelos que tengan coeficientes con un aporte estadísticamente significativo. Este procedimiento es conocido como sobreparametrización. Dependiendo de la cantidad de observaciones y del rango con que se trabajen los coeficientes, la comparación de los modelos puede volverse muy extensa y complicada, razón por la cual resulta imperativo generar un procedimiento sistemático que logre seleccionar el mejor modelo con base en sus medidas de ajuste y rendimiento.

Es aquí donde entra en escena el análisis combinatorio, pues a partir de sus procedimientos es posible conocer la cantidad de modelos que deben ser probados. Resulta pertinente discutir dos principios fundamentales del análisis combinatorio mencionados por @analisis_combinatorio: Uno es el principio de adición, el cual indica que si se tienen dos procedimientos $A$ y $B$, los cuales pueden realizarse de $k_A$ y $K_B$ maneras, respectivamente, entonces la cantidad de maneras que se puede realizar uno u otro procedimiento es $k_A + k_B$. Por otro lado se tiene el principio de multiplicación, con el cual, si el procedimiento $A$ se puede realizar de $k_A$ formas distintas, seguido de otro procedimiento $B$ que puede realizarse de $k_B$ formas, entonces si a cada forma de realizar el procedimiento $A$ se puede asociar a cualquiera de las $k_B$ maneras de realizar el procedimiento $B$, entonces ambos procedimientos pueden realizarse de $k_A \cdot k_B$ formas distintas.

Es a partir de estos dos principios que pueden obtenerse la cantidad de formas distintas que pueden ordenarse $m$ elementos tomando $r$ elementos a la vez. Uno de ellos son las permutaciones, descritos en la ecuación \ref{eqn:permutacion}, la cual describe la forma de calcular la cantidad de formas distintas que puede ordenarse $m$ elementos tomando $r$ a la vez, donde el orden sí importa, a modo de ejemplo, si se quiere saber la cantidad formas que pueden ordenarse las letras $A, B$ y $C$ tomando dos letras a la vez, se tendría que existen $\frac{3!}{(3-1)!}=6$ formas distintas, que son $AB, AC, BC, BA, CA$ y $CB$. De manera similar, se tienen las combinaciones, cuya fórmula se describe en la ecuación \ref{eqn:combinacion}, que brinda la cantidad de maneras distintas en que pueden ordenarse $m$ elementos tomando $r$ a la vez donde el orden no importa; es decir, si se desean ordenar las letras $A, B$ y $C$ tomando dos a la vez, se tendrían $\frac{3!}{2!(3-1)!}=3$ formas distintas, las cuales son $AB, AC$ y $BC$.

\begin{equation}
\label{eqn:permutacion}
_mP_r=\frac{m!}{(m-r)!}
\end{equation}

\begin{equation}
\label{eqn:combinacion}
_mC_r=\frac{m!}{r!(m-r)!}
\end{equation}

Es a partir de esto que la sobreparametrización se utiliza en conjunto con el análisis combinatorio y en particular con el método de permutaciones, pues el orden de la cantidad de coeficientes a estimar en un modelo $ARIMA(p,d,q)$ sí importa, debido a que no es lo mismo estimar un modelo $ARIMA(2,1,3)$ que un modelo $ARIMA(3,1,2)$. En las elección de modelos ARIMA normalmente los métodos tradicionales como los correlogramas u otros, no suelen abarcar un espectro más amplio de coeficientes, y esto podría representar un método de estimación que no es el mejor, por esto, la presente tesis propone una  metodología que mezcla la sobreparametrización con las permutaciones con el objetivo de lograr estimar el mejor modelo $ARIMA$ de una amplia cantidad de posibles candidatos para conseguir pronósticos más precisos en comparación a los métodos tradicionales.
