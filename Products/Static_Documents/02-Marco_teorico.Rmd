Los modelos de series cronológicas han sido un importante tema de investigación durante décadas [@tsa_decades]. Su objetivo principal consiste en obtener simplificaciones de la realidad mediante el ajuste de diversos modelos, los cuales se ajustan a datos recolectados a lo largo del tiempo de forma periódica.  

Sin embargo, encontrar un modelo que presente un buen comportamiento con respecto a los datos no es sencillo, pues deben considerarse diversos aspectos teóricos, prácticos, y de la temática de estudio para así obtener un modelo adecuado que logre generar pronósticos realistas y pertinentes para la toma de decisiones [@tsa_decision_making].

Una serie temporal se define como una secuencia de datos observados, cuyas mediciones ocurren de manera sucesiva durante un periodo de tiempo. Los registros de estos datos pueden referirse a una única variable en cuyo caso de dice que es una serie univariada; o bien, pueden registrarse distintas variables para el mismo periodo de tiempo, conocida como serie temporal multivariada. Según @Hipel, cada observación puede ser continua o discreta, como la temperatura de una ciudad durante el día o las variaciones diarias del precio de un activo financiero, respectivamente; las observaciones continuas, además, pueden ser convertidas a su vez en observaciones discretas. 

El presente capítulo consta de seis apartados. El primer apartado abarca los cuatro componentes de una serie cronológica. Posteriormente, la segunda sección repasa los supuestos fundamentales en el análisis de series cronológicas. Con los elementos más básicos introducidos, el tercer apartado cubre el eje central de esta investigación: Los modelos Autorregresivos Integrados de Medias Móviles y sus componentes, los modelos autorregresivos y los modelos de medias móviles, así como la metodología Box-Jenkins y el proceso para la identificación de los modelos. En el cuarto apartado se introducen los métodos para la identificación de los modelos. El quinto apartado abarca los componentes relacionados a los autocorrelogramas, la forma más difundida para la selección de modelos y, finalmente, el sexto apartado introduce el principal aporte de este estudio, la sobreparametrización como método para la selección de modelos.

\subsection{Componentes de una serie cronológica}

En el análisis de series cronológicas existen dos grandes corrientes de estudio: Los componentes inherentes a la serie cronológica y el estudio de las autocorrelaciones. Según el primer enfoque, de acuerdo con @oscarh-1, las series cronológicas poseen tres componentes principales: Tendencia-ciclos, Estacionalidad e Irregularidad. Considerando estos tres elementos, las series cronológicas pueden ser *aditivas*, como se muestra en la ecuación \ref{eqn:serie_aditiva}, en cuyo caso se asume que los tres componentes son independientes entre sí; o *multiplicativa*, donde, por el contrario, los tres componentes no son independientes, como muestra la ecuación \ref{eqn:serie_multiplicativa}.

\begin{equation}
\label{eqn:serie_aditiva}
Y(t)=T(t)+S(t)+I(t)
\end{equation}

\begin{equation}
\label{eqn:serie_multiplicativa}
Y(t)=T(t)\times S(t)\times I(t)
\end{equation}

Donde $Y$ es la serie cronológica, $T$ es la tendencia-ciclo, $S$ es la parte estacional, $I$ la parte irregular o aleatoria, y $t$ es el momento en el tiempo. Esta perspectiva clásica del análisis de series de tiempo permite realizar un análisis descriptivo del comportamiento de la serie en cuestión; cada una de sus partes se definen en posteriores apartados. De manera visual, una serie cronológica aditiva posee un comportamiento similar al mostrado en la figura \ref{fig:ejemplo_aditiva}, mientras que un comportamiento multiplicativo puede apreciarse en la figura \ref{fig:ejemplo_multiplicativa}.

```{r ejemplo_aditiva, fig.cap="Número de matrimonios en Costa Rica para el periodo 1978-1983"}
library(dplyr)
library(ggplot2)
library(ggpmisc)

TS <- ts(c(2522,2750,2210,3154,2392,2288,2800,2178,2316,1782,2068,3436,2344,2980,3334,2184,2212,2838,2560,2380,2254,1956,2270,3410,2264,3144,3400,2260,2924,2440,2550,2788,2114,2068,2516,3488,2320,2858,2716,2340,2638,2374,2550,2746,2160,2388,2194,3010,2504,2898,3124,2544,3010,2494,3330,2494,2278,2446,2436,3676,2630,2928,3218,3012,2794,2778,3322,2734,2600,2734,2664,3546), 
         start = c(1978,1), end = c(1983, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: UED-INEC",
       y="Matrimonios",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))# +
  #scale_x_discrete(expand = c(0,0), limits=c(1995, 2005, 2015))
```

```{r ejemplo_multiplicativa, fig.cap="Número de turistas en Costa Rica para el periodo 1991-2000"}
TS <- ts(c(47204, 45584, 48206, 36619, 31607, 36859, 47855, 45846, 33029, 37674, 43416, 50750, 58361, 58168, 54148, 45539, 41879, 46124, 57957, 52587, 36224, 40615, 51978, 67011, 70545, 69626, 63736, 53469, 43217, 47328, 66448, 55842, 42217, 45353, 56181, 70043, 83163, 73510, 78045, 57318, 48028, 50142, 67431, 64030, 49012, 52665, 61670, 76434, 88908, 76361, 72734, 60732, 52293, 54564, 70297, 61089, 49291, 51141, 68329, 78871, 90627, 80543, 78923, 60261, 50696, 57057, 66121, 60594, 44720, 48271, 62856, 80458, 91584, 80709, 77573, 58597, 54849, 60822, 74928, 62568, 50868, 54933, 62692, 81367, 101145, 89743, 89327, 78634, 64476, 71379, 85030, 72376, 56949, 64173, 72802, 96819, 117108, 98694, 102553, 81663, 69663, 76924, 92211, 80765, 59367, 66001, 81174, 105462, 115990, 106290, 107929, 87931, 75436, 77011, 91906, 78326, 65258, 68832, 93995, 119171), 
         start = c(1991,1), end = c(2000, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  # stat_smooth(color = "#FC4E07", fill = "#FC4E07",
  #             method = "loess")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: UED-INEC",
       y="Turistas",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10))) +
  scale_x_discrete(expand = c(0,0), limits=c(1991:2000))
```

\subsubsection{La tendencia-ciclo}

A partir del texto de @calderon2012estadistica, la tendencia general de una serie cronológica se refiere al crecimiento, decrecimiento o lateralización de sus movimientos a lo largo del periodo de estudio. Un ejemplo es la serie cronológica del número de matrimonios en Costa Rica para el periodo 1978-1983, que con el tiempo su crecimiento suele comportarse de una forma creciente tal y como muestra la figura \ref{fig:ejemplo_tendencia}. 

```{r ejemplo_tendencia, fig.cap="Tendencia del número de matrimonios en Costa Rica para el periodo 1978-1983"}
TS <- ts(c(2522,2750,2210,3154,2392,2288,2800,2178,2316,1782,2068,3436,2344,2980,3334,2184,2212,2838,2560,2380,2254,1956,2270,3410,2264,3144,3400,2260,2924,2440,2550,2788,2114,2068,2516,3488,2320,2858,2716,2340,2638,2374,2550,2746,2160,2388,2194,3010,2504,2898,3124,2544,3010,2494,3330,2494,2278,2446,2436,3676,2630,2928,3218,3012,2794,2778,3322,2734,2600,2734,2664,3546), 
         start = c(1978,1), end = c(1983, 12), frequency = 12)
#Se requiere el paquete ggpmisc
ggplot(TS)+
  geom_line(color = "#00AFBB", size = 1.3) +
  stat_smooth(color = "#FC4E07", fill = "#FC4E07",
              method = "lm")+
  #geom_vline(xintercept = 2000.6, linetype="dashed", color = "red")+
  theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: UED-INEC",
       y="Matrimonios",
       x="Año") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))
```

Del informe elaborado también por @calderon2012estadistica se desprende que los periodos cíclicos, por su parte, se refieren a los cambios que se dan en una serie cronológica en el mediano-largo plazo, que son causados por determinados eventos que suelen repetirse. Estos ciclos suelen tener una duración determinada, como es el caso de los índice bursátil NASDAQ-100. Este indicador resume el estado los 100 valores de las compañías más importantes del sector de la industria de la tecnología, y sus ciclos suelen presentar un auge, seguido por un descenso que, posteriormente, se vuelve una depresión, y que finalmente se convierte en una recuperación a su estado inicial. La figura \ref{fig:ejemplo_ciclo} muestra como el índice NASDAQ-100 inicia un auge alrededor de enero de 1995 (primera línea azul punteada), para luego experimentar una fuerte caída a partir de junio del año 2000 (línea roja punteada) y posteriormente iniciar un periodo de recuperación en enero del año 2009 (segunda línea azul punteada).

```{r ejemplo_ciclo, fig.cap="Índice bursatil NASDAQ-100 para el periodo enero 1990 - junio 2021", message=FALSE, }
ggplot(log(ts(readr::read_csv(read_from_dir("^NDX.csv", "Data/Raw"))$Close, 
   start = c(1990, 1), end = c(2021, 6), frequency = 12))) +
  geom_line(color = "#00AFBB", size = 1.3) +
  geom_vline(xintercept = c(1995.1, 2009.1, 2000.6), linetype="dashed", color = c("blue", "blue", "red"))+
   theme_minimal() +
  theme(text = element_text(size=9),
        axis.text.x = element_text(angle=0, hjust=1)) +
  labs(caption="Fuente: Yahoo Finance",
       y="Precios de cierre (escala logarítmica)",
       x="Periodo") +
  theme(plot.title = element_text(hjust = 0.5, face="plain"),
        plot.caption=element_text(hjust=0, vjust=0.5,
                                  margin=margin(t=1,10,10,10)))
```

La descomposición clásica de la tendencia-ciclo que este componente se mantiene constante de un periodo al siguiente y se obtiene a partir de una media móvil de $m$ periodos $\left(\bar y_{m}\right)$. De esta manera la forma matemática de la tendencia-ciclo para una serie cronológica se muestra en la ecuación \ref{eqn:descomposicion_tendencia}.

\begin{equation}
\label{eqn:descomposicion_tendencia}
T(t)=
\begin{cases}
2\bar y_{m}, & \text{si}\ \text{m es par} \\
\bar y_{m}, & \text{si}\ \text{m es impar} \\
\end{cases}
\end{equation}


\subsubsection{Componentes estacionales}

@calderon2012estadistica también se refiere a los cambios estacionales que se presentan en una serie de tiempo, los cuales se relacionan con las fluctuaciones naturales del fenómeno dentro de una temporada de observaciones. Visualmente los efectos estacionales puede apreciarse en la figura \ref{fig:ejemplo_multiplicativa}, en donde los picos más altos de turistas siempre se ubican entres los meses de diciembre y enero. Matemáticamente, el componente estacional puede definirse como se indica en la ecuación \ref{eqn:descomposicion_estacional}:

\begin{equation}
\label{eqn:descomposicion_estacional}
S(t)=\bar{y}_{s_t}-\bar{y}_{k};
\begin{cases}
\bar{y}_{s_t} = \frac{\sum \bar{y}_s}{n} \\
\bar{y}_k = \frac{\sum y_k}{n-m} \\
\bar{y}_s = \sum_{j=1}^s y_{k_j} \\
y_k = y_t - \bar{y}_{mc_t} \\
\bar{y}_{mc_t} = \frac{\bar{y}_{m_t} + \bar{y}_{m_{t-1}}}{2} \\
\bar{y}_{m_t} = \frac{\sum_{t=1}^m y_t}{m} \\
\end{cases}
\end{equation}

donde $m$ representa la cantidad de periodos y $s$ la frecuencia estacional.

\subsubsection{Componente irregular}

Finalmente, la irregularidad de una serie cronológica, siguiendo a @calderon2012estadistica, se refiere a las fluctuaciones propias de un fenómeno que no pueden ser predichas. Estos cambios no se dan de manera regular, es decir, no siguen un patrón determinado. Matemáticamente su descomposición se obtiene a partir de los otros componentes así como de la propia serie cronológica $y(t)$, tal y como se muestra en la ecuación \ref{eqn:descomposicion_irregular}.

\begin{equation}
\label{eqn:descomposicion_irregular}
I(t)=
\begin{cases}
y(t)-T(t)-S(t), & \text{si}\ \text{la serie es aditiva} \\
\frac{y(t)}{T(t)S(t)} , & \text{si}\ \text{la serie es multiplicativa} \\
\end{cases}
\end{equation}

\subsection{Supuestos en el análisis de series cronológicas}

PROCESO ESTOCASTICO: http://www.ccs.fau.edu/~bressler/EDU/STSA/Modules/I.pdf

El análisis de series temporales, según @Hipel, representa un método para comprender la naturaleza de la serie en cuestión y poder utilizarla para generar pronósticos. Es en este sentido que entran en escena las observaciones recolectadas de la serie, pues ellas son analizadas y sujetas a modelados matemáticos que logren capturar el proceso que gobierna a toda la serie cronológica [@Zhang]. Los pronósticos se generan a partir de este modelo, es decir, pronosticar el futuro, se utilizan las correlaciones con las observaciones pasadas.

En un proceso determinístico, es posible predecir con certeza lo que ocurrirá en el futuro; las series cronológicas, sin embargo, carecen de esta condición. El análisis de series cronológicas asume que las observaciones pueden ajustarse a un determinado modelo estadístico, esto se conoce como un proceso estocástico. Es de esta manera que @Hipel sugieren que una serie cronológica puede considerarse como una muestra aleatoria de una serie mucho más grande.

Como una serie de tiempo puede considerarse como un proceso estocástico, éstas se encuentran sujetas a múltiples supuestos. El más fundamental de ellos es que todas las observaciones son independientes e idénticamente distribuidas (i.i.d.) siguiendo una distribución aproximadamente Normal, con una media y variancia dadas. Lo anterior es contrario al uso de las observaciones pasadas para pronosticar el futuro, por lo que este supuesto, según @Cochrane, no es exacto pues una una serie de tiempo no es exactamente, i.i.d., sino que siguen un patrón medianamente regular en el largo plazo.

Otro concepto de interés en las series cronológicas es el de estacionaridad. De acuerdo con @stationary_def, una serie se considera estacionaria cuando su nivel medio y su variancia son aproximadamente las mismas durante todo el periodo, es decir, el tiempo no afecta a estos estadísticos de variabilidad. Este supuesto busca simplificar la identificación del proceso estocástico con el objetivo de obtener un modelo adecuado para generar los pronósticos. Sin embargo, y de una manera similar al supuesto de i.i.d., si una serie cronológica posee tendencias o patrones estacionales hace que esta sea no estacionaria. En la práctica, una serie puede volverse estacionaria al aplicarle transformaciones o diferenciaciones de distinto orden.

El último supuesto, y quizá el que más debate genera, es el criterio de parsimonia. Como mencionan @Zhang y @Hipel, este principio sugiere que se prioricen modelos sencillos, con pocos parámetros, para representar una serie de datos. Mientras más grande y complicado sea el modelo, mayor será el riesgo de sobre ajuste, lo que implica que el ajuste sea muy bueno en el conjunto de datos con que se generó el modelo, pero que los pronósticos generados sean pobres ante nuevos conjuntos de datos. Este problema, sin embargo, se presenta al considerar un único modelo con muchos parámetros; pero si se consideran varios modelos y estos son sometidos a distintos criterios, puede obtenerse un modelo sobreparametrizado que ofrezca buenos pronósticos.

\subsection{Modelos Autorregresivos Integrados de Medias Móviles}

Hay dos grandes grupos de modelos lineales de series cronológicas: Los modelos Autorregresivos (AR) [@Lee] y los modelos de Medias Móviles (MA) [@box-jenkins]. La combinación de estos dos grandes grupos forman los Modelos Autorregresivos de Medias Móviles (ARMA) [@Hipel] y los modelos Autorregresivos Integrados de Medias Móviles (ARIMA), siendo este último de particular interés en esta investigación.

Los modelos ARIMA son los de uso más extendido en el análisis de series cronológicas. Se fundamentan en las autocorrelaciones pasadas, y contempla un proceso iterativo para identificar un posible proceso óptimo a partir de una clase general de modelos. El teorema de Wold [@Wold] sugiere que todo proceso estacionario puede ser determinado de una forma específica y cuya ecuación posee, en realidad, infinitos coeficientes, pero que debe ser reducido a una cantidad finita para luego evaluar su ajuste sometiéndolo a diferentes pruebas y medidas de rendimiento.

\subsubsection{Ecuación de Wold}

Según @sargent_macro, cualquier proceso estacionario puede ser representado mediante la ecuación \ref{eqn:proceso_estacionario}:

\begin{equation}
\label{eqn:proceso_estacionario}
x_t=\sum_{j=0}^{\infty} \psi_j\varepsilon_{t-j}+\kappa_t
\end{equation}

donde $\forall \psi_j \in \mathbb{R}, \psi_0=1, \sum_{j=0}^{\infty} \psi_j^2<\infty$, y $\varepsilon_t$ representa un ruido blanco i.i.d., es decir, $\varepsilon_t \sim N(0, \sigma^2)$; además, $\kappa_t$ es el componente lineal determinístico de forma tal que $cov(\kappa_t,\varepsilon_{t-j}=0)$, lo cual implica que este componente determinístico es independiente de la suma infinita de los choques pasados.

De lo anterior, si se omite la parte determinística $\kappa_t$ de \ref{eqn:proceso_estacionario}, el remanente es la suma ponderada infinita, lo cual implica que si se conocen los ponderadores $\psi_j$, y si además se conoce $\sigma_\varepsilon^2$, es posible obtener una representación para cualquier proceso estacionario; este concepto es conocido como *media móvil infinita*.

Sabiendo que $\varepsilon_t \sim N(0, \sigma^2)$, se tiene que $\varepsilon_t$ tiene media 0, es decir, está centrado en este valor. De esta manera el ruido blanco es por definición un proceso centrado, lo cual implica que la suma ponderada infinita está centrada en sí misma. De esta manera, la representación de Wold de un proceso $x_t$ supone que se suman los choques pasados más un componente determinístico que no es otro que el valor esperado del proceso: $\kappa_t=m$, donde $m$ es una constante cualquiera. Así, la ecuación \ref{eqn:proceso_estacionario} puede sustuirse por:

\begin{equation}
\label{eqn:proceso_estacionario2}
x_t=\sum_{j=0}^{\infty} \psi_j\varepsilon_{t-j}+m
\end{equation}

y de \ref{eqn:proceso_estacionario2} puede verificarse que,

\begin{equation}
\label{eqn:dem_proceso_estacionario2}
E(x_t)=E\left(\sum_{j=0}^{\infty} \psi_j\varepsilon_{t-j}+m\right)=\sum_{j=0}^{\infty} \psi_jE\left(\varepsilon_{t-j}\right) + m = m
\end{equation}

La principal consecuencia del teorema de Wold es que, si se conocen los ponderadores $\psi_j$, y además $\sigma_\varepsilon^2$ es ruido blanco es posible conocer el proceso por medio del cual se rige la serie cronológica. Esto permite realizar cualquier previsión, denotada por $\hat X_{T+h}$ para el proceso de interés $x_T$ en el momento $T+h$ para una muestra cualquiera de $T$ observaciones de $x_t$. De acuerdo con @sargent_macro, basado en el teorema de Wold, la mejor previsión posible para un proceso $x_t$ para el momento $T+h$, denotado por $\hat x_{T+h}$, la predicción está dada por:

\begin{equation}
\label{eqn:prevision}
\hat x_{T+h}=\sum_{j=1}^{\infty} \psi_j \varepsilon_{T-j+1}
\end{equation}

De la ecuación \ref{eqn:prevision} se desprende que el error de previsión asociado está dado por:

\begin{equation}
\label{eqn:error_prevision}
x_{T+h}- \hat x_{T+h}=\sum_{j=1}^{\infty} \psi_j \varepsilon_{T-h+1}
\end{equation}

\subsubsection{Modelos Autorregresivos}

Un modelo autorregresivo de orden *p*, denotado como $AR(p)$, considera los valores futuros de una serie cronológica como una combinación lineal las $p$ observaciones predecesoras, un componente aleatorio y un término constante. @Hipel y @Lee emplean la notación de la ecuación \ref{eqn:modelo_AR}.

\begin{equation}
\label{eqn:modelo_AR}
y_t=c+\sum_{i=1}^p \varphi_iy_{t-i}+\varepsilon_t
\end{equation}

Donde $y_t$ y $\varepsilon_t$ corresponden al valor de la serie y al componente aleatorio en el momento actual $t$, mientras que $\varphi_i$, con $i=1,2,\cdots,p$ son los parámetros del modelo, y $c$ es su término constante, que en ciertas ocasiones se suele omitir para simplificar la notación. Los parámetros de esta clase de modelos suelen estimarse mediante la ecuación de Yule-Walker [@yule.walker].

\subsubsection{Modelos de Medias Móviles}

De manera similar a como un $AR(p)$ utiliza los valores pasados para pronosticar los futuros, los modelos de medias móviles de orden q, denotados como $MA(q)$, utilizan los errores pasados de las variables independientes. Estos modelos se describen mediante la ecuación \ref{eqn:modelo_MA}.

\begin{equation}
\label{eqn:modelo_MA}
y_t=\mu+\sum_{j=1}^q \theta_j \varepsilon_{t-j}+\varepsilon_t
\end{equation}

Donde $\mu$ representa el valor medio de la serie cronológica y cada valor de $\theta_j(j=1,2,\cdots,q)$ son los parámetros del modelo. Como los $MA(q)$ utilizan los errores pasados de la serie cronológica, se asume que estos son i.i.d. centrados en cero y con una variancia constante, siguiendo una distribución aproximadamente Normal, con lo cual este tipo de modelos pueden considerarse como una regresión lineal entre una observación determinada y los términos de error que le preceden [@stationary_def]. 

\subsubsection{Metodología Box-Jenkins}

La combinación de un $AR(p)$ y un $MA(q)$, descritos en las ecuaciones \ref{eqn:modelo_AR} y \ref{eqn:modelo_MA} respectivamente, como se mencionó al inicio de esta sección, generan los modelos autorregresivos de medias móviles, $ARMA(p,q)$, representados mediante la ecuación \ref{eqn:modelo_ARMA}.

\begin{equation}
\label{eqn:modelo_ARMA}
y_t=c+\varepsilon_t+\sum_{i=1}^p \varphi_iy_{t-i}+\sum_{j=1}^q \theta_j \varepsilon_{t-j}
\end{equation}

@Cochrane menciona que los modelos $ARMA(p,q)$ suelen manipularse mediante lo que se conoce como operador de rezagos, denotado como $Ly_t=y_{t-1}$. Esto significa que en un $AR(p)$ se tiene que $\varepsilon_t=\varphi(L)y_t$, mientras que en $MA(q)$ se tiene que $y_t=\theta(L)\varepsilon_t$, y por consiguiente en un $ARMA(p,q)$ se tiene $\varphi(L)y_t=\theta(L)\varepsilon_t$. Por lo tanto, de lo anterior se desprende que $\varphi(L)=1-\sum_{i=1}^p \varphi_iL^i$, y que $\theta(L)=1+\sum_{j=1}^q\theta_jL^j$.

Los modelos $ARMA$, sin embargo, solamente pueden ser utilizados en series cronológicas suyo proceso es estacionario. Esto, en la práctica, es poco común, pues una serie de tiempo a menudo posee tendencias y ciertos patrones estacionales y, además, como menciona @Hamzacebi, presentan procesos no estacionarios por naturaleza. Esta condición hace necesaria la introducción de una generalización de los modelos $ARMA$, la cual se conoce como los modelos $ARIMA$ [@box-jenkins].

\subsubsection{Modelos ARIMA}

Partiendo de una serie con un proceso no estacionario, es posible aplicar transformaciones o diferenciaciones (*d*)a los datos con el objetivo de convertirlos en un proceso estacionario. Utilizar la notación de rezagos descrita anteriormente, según @Lombardo, permite plantear un modelo $ARIMA(p,d,q)$ como se describe en la ecuación \ref{eqn:modelo_ARIMApdq}.

\begin{equation}
\label{eqn:modelo_ARIMApdq}
\varphi(L)(1-L)^dy_t=\theta(L)\varepsilon_t\\
\left(1-\sum_{i=1}^p \varphi_iL^i \right)(1-L)^d y_t=\left(1+\sum_{j=1}^q \theta_jL^j \right) \varepsilon_t
\end{equation}

Donde los términos $p, d$ y $q$ son positivos y mayores a cero y corresponden al modelo autorregresivo, a la diferenciación y al modelo de medias móviles, respectivamente. El componente $d$ es el número de diferenciaciones, si $d=0$ se tiene un modelo ARMA, y $d\geq1$ representa el número de diferenciaciones; en la mayoría de casos $d=1$ suele ser suficiente. Así, un $ARIMA(p,0,0)=AR(p)$, $ARIMA(0,0,q)=MA(q)$, y un $ARIMA(0,1,0)=y_t=y_{t-1}+\varepsilon_t$, es decir, un modelo de caminata aleatoria.

Como sugieren @box-jenkins, lo anterior puede generalizarse aún más al considerar los efectos estacionales de la serie cronológica. Si se considera una serie cronológica con observaciones mensuales, una diferenciación de primer orden es igual a la diferencia entre una observación y la observación correpondiente al mismo mes pero del año anterior; es decir, si el periodo estacional es de $s=12$ meses, entonces esta diferencia estacional aplicada a un $ARIMA(p,d,q)(P,D,Q)_S$ es calculada mediante $z_t=y_t-y_{t-s}$. 

De esta manera, el método de @box-jenkins inicia con el análisis exploratorio de la serie cronológica, teniendo un interés particular en identificar si hay presencia de factores no estacionarios en la misma. Si en efecto se cuenta con una serie no estacionaria, ésta debe volverse estacionaria mediante algún tipo de transformación, típicamente el logaritmo natural. Con la serie ya transformada, se busca identificar el proceso que gobierna la serie. La forma clásica de hacer esto es mediante los gráficos de autocorrelación y autocorrelación parcial. Cuando se logra identificar un proceso que se adecue más a la serie cronológica, se deben realizar los diagnósticos para evaluar la calidad del ajuste del modelo, así como las medidas de rendimiento referentes a los pronósticos que genera el modelo estimado hasta un horizonte determinado.

\subsection{Identificación del modelo}

Los métodos más clásicos para la identificación del proceso que gobierna a una serie cronológica son las funciones de autocorrelación y autocorrelación parcial, las cuales sirven de indicador acerca de qué tan relacionadas están las observaciones unas de otras. Estas funciones ofrecen indicios sobre el orden de los términos para los modelos $AR(p)$, $MA(q)$ y para la diferenciación y, por ende, para la identificación de un modelo $ARIMA$ [@hyndman_box-jenkins].

Para medir la relación lineal entre dos variables cuantitativas es común utilizar el coeficiente de correlación $r$ de Pearson [@pearson], el cual se define para dos variables $X$ e $Y$ como se muestra en la ecuación \ref{eqn:pearson}.

\begin{equation}
\label{eqn:pearson}
r_{X,Y}=\frac{E(XY)}{\sigma_X \sigma_Y} = \frac{\sum_{i=1}^n \left(X_i- \bar X\right) \left(Y_i- \bar Y\right)}{\sqrt{\sum_{i=1}^n \left(X_i- \bar X\right)^2 \sum_{i=1}^n \left(Y_i- \bar Y\right)^2}}
\end{equation}

Este mismo concepto puede aplicarse a las series cronológicas para comparar el valor de la misma en el tiempo $t$, con su valor en el tiempo $t-1$, es decir, se comparan las observaciones consecutivas $Y_t$ con $Y_{t-1}$. Esto también es aplicable a no solo una observación rezagada $(Y_{t-1})$, sino también con múltiples rezagos $(Y_{t-2}), (Y_{t-3}), \cdots,(Y_{t-n})$. Para esto se hace uso del coeficiente de autocorrelación.

El coeficiente de autocorrelación (*ACF* por sus siglas en inglés) recibe su nombre debido a que se utiliza el coeficiente de correlación para pares de observaciones $r_{Y_t, Y_{t-1}}$ de la serie cronológica. Al conjunto de todas las autocorrelaciones se le llama función de autocorrelación.

La función de autocorrelación parcial[^5], como menciona @oscarh-4, busca medir la asociación lineal entre las observaciones $Y_t$ y $Y_{t-k}$, descartando los efectos de los rezagos $1,2, \cdots ,k-1$.

[^5]: *PACF* por sus siglas en inglés

Cuando se tiene el modelo ARIMA debidamente identificado, es importante realizar los pronósticos. Sin embargo, estos pronósticos no son imperativos, sino que se debe evaluar su calidad con las llamadas medidas de rendimiento. Estas mediciones son hechas comparando el pronóstico y su diferencia con el valor real. Existen múltiples medidas de rendimiento, @medidas menciona entre ellas el *MAE*, *MAPE*, *RMSE*, *MASE*, *AIC*, *AICc* y el *BIC*.

\subsection{Los autocorrelogramas}
```{r, eval=FALSE}
#Lo siguiente se obtuvo del sitio:
#https://people.duke.edu/~rnau/arimrule.htm
```

El uso del *ACF* y el *PACF* se suele aplicar de manera visual. Sin embargo, hacer usos de estos elementos implica considerar múltiples condiciones. En el caso de la identificación del orden de la diferenciación:

 - Si la serie posee autocorrelaciones positivas en un amplio número de rezagos, entonces es posible que se requiera un orden más alto en el valor de $d$.
 - Si la autocorrelación en $t-1$ es menor o igual a cero, o si las autocorrelaciones resultan ser muy bajas y sin seguir algún patrón en particular, entonces no se requiere un alto orden para la diferenciación.
 - Una desviación estándar baja suele ser indicador de un orden adecuado de integración.
 - Si no se utiliza ninguna diferenciación, se asume que la serie cronológica es estacionaria. Aplicar una diferenciación asume que la serie cronológica posee una media constante, mientras que dos diferenciaciones sugiere que la tendencia varía en el tiempo.

Para la identificación de los términos $p$ y $q$:

 - Si la *PACF* de la serie cronológica diferenciada muestra una diferencia marcada y si, además, la autocorrelación en $t-1$ es positiva, entonces debe considerarse aumentar el valor de $p$.
 - Si la *PACF* de la serie cronológica diferenciada muestra una diferencia marcada y si, además, y la autocorrelación en $t-1$ es negativa, entonces debe considerarse aumentar el valor de $q$.
 - Los términos $p$ y $q$ pueden cancelar sus efectos entre sí, por lo que si se cuenta con un modelo $ARMA$ más mixto que parece adaptarse bien a los datos, puede deberse también a que $p$ o $q$ deben ser menores.
 - Si la suma de los coeficientes del modelo $AR$ es muy cercana a la unidad, es necesario reducir la cantidad de términos en uno y aumentar el orden de la diferenciación en uno.
 - Si la suma de los coeficientes del modelo $MA$ es muy cercana a la unidad, es necesario reducir la cantidad de términos en uno y disminuir el orden de la diferenciación en uno.

Tener en consideración estos y otros posibles criterios para la identificación del proceso que gobierna la serie cronológica puede fácilmente volverse algo subjetivo, pues dos personas diferentes pueden llegar a dar distintas interpretaciones a las visualizaciones de los autocorrelogramas. Estas interpretaciones pueden sesgar la identificación de los modelos y, además, no considerar otros escenarios para los términos de un modelo $ARIMA$; para solventar esto es necesario considerar un abanico más amplio de opciones que a su vez elimine el criterio subjetivo del observador, lo cual se puede lograr al considerar múltiples permutaciones de términos, es decir, empleando la sobreparametrización.

\subsection{La sobreparametrización y el análisis combinatorio}

La identificación visual mediante los autocorrelogramas puede llevar a decisiones erradas acerca del proceso que gobierna la serie cronológica. Una alternativa es considerar estimaciones procesos de ordenes bajos, como un $ARMA(1,1)$ y poco a poco ir incorporando términos, este proceso de revisión permite encontrar los puntos en que agregar un coeficiente más al modelo no aporta ninguna mejora en los resultados del pronóstico, y así considerar únicamente aquellos modelos que tengan coeficientes con un aporte estadísticamente significativo. Este procedimiento es conocido como sobreparametrización. Dependiendo de la cantidad de observaciones y del rango con que se trabajen los coeficientes, la comparación de los modelos puede volverse muy extensa y complicada, razón por la cual resulta imperativo generar un procedimiento sistemático que logre seleccionar el mejor modelo con base en sus medidas de ajuste y rendimiento del modelo.
